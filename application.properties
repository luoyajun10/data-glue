#to_kudu,to_hbase,to_es
syn_flag = to_hbase

# mysql
meta.mysql.jdbc.url = jdbc:mysql://localhost:3306/data-integration
meta.mysql.jdbc.user = dev
meta.mysql.jdbc.password = dev123

# kafka
kafka.broker.list = kafka-broker-01:9092,kafka-broker-02:9092,kafka-broker-03:9092
kafka.consumer.topics = RDS_PCLOUD_PROD
kafka.consumer.group.id = rds_kafka_test_group_a
kafka.consumer.client.id = rds_kafka_test_group_a
kafka.consumer.auto.offset.reset = largest
kafka.consumer.socket.timeout.ms = 30000
kafka.consumer.socket.receive.buffer.bytes = 65536
kafka.consumer.enable.auto.commit = false

# zookeeper
zookeeper.connect = hadoop-master-01:2181,hadoop-master-02:2181,hadoop-master-03:2181
zookeeper.session.timeout.ms = 30000
zookeeper.connection.timeout.ms = 10000

# kudu
#kudu.master = hadoop-master-01:7051,hadoop-master-02:7051,hadoop-master-03:7051
#kudu.database = DB1.
#kudu.session.buffer.space = 500
#kudu.session.timeout.ms = 30000

# hbase
#hbase.zookeeper.quorum = hadoop-master-01,hadoop-master-02,hadoop-master-03
#hbase.zookeeper.port = 2181
#hbase.namespace = DB1:
#hbase.write.buffer.size = 100

# es
#es.httpHost.host1=xx.xxx.xxx.xx
#es.httpHost.host2=xx.xxx.xxx.xx
#es.httpHost.host3=xx.xxx.xxx.xx
#es.httpHost.host4=xx.xxx.xxx.xx
#es.httpHost.host5=xx.xxx.xxx.xx
#es.httpHost.host6=xx.xxx.xxx.xx
#es.httpHost.port1=9200
#es.httpHost.port2=9200
#es.httpHost.port3=9200
#es.httpHost.port4=9200
#es.httpHost.port5=9200
#es.httpHost.port6=9200
#es.httpHost.schema=http

# mysql
#mysql.jdbc.url = jdbc:mysql://localhost:3306/db1
#mysql.jdbc.user = root
#mysql.jdbc.password = root123
#mysql.write.buffer.size = 200

# spark
spark.streaming.batch.duration.seconds = 2
spark.streaming.kafka.maxRatePerPartition = 1000